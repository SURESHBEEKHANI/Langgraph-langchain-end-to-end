{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Cu_pYwIKAUs"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
        "LANGSMITH_TRACING=os.getenv(\"LANGSMITH_TRACING\")\n",
        "LANGSMITH_API_KEY=os.getenv(\"LANGSMITH_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LW2FMfFP_OQM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"LANGSMITH_TRACING\"]=LANGSMITH_TRACING\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=LANGSMITH_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2qUMkyoc85cF"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "model=ChatGroq(model_name=\"Gemma2-9b-It\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrWr856dUYZ0",
        "outputId": "2d29db2c-7557-43b1-fb1c-6995cef078fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Not much, just hanging out in the digital world, waiting to be helpful! What about you? What's going on in your world today? 😊\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 16, 'total_tokens': 50, 'completion_time': 0.061818182, 'prompt_time': 7.534e-05, 'queue_time': 0.020891768, 'total_time': 0.061893522}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-22ca266d-5b63-43e9-a3a0-6e95e67f6e48-0', usage_metadata={'input_tokens': 16, 'output_tokens': 34, 'total_tokens': 50})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.invoke(\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "41gGc9AyIOnz"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kVGDaPvPGESO"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VLErHCqoLPCG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SURESH BEEKHANI\\AppData\\Local\\Temp\\ipykernel_1452\\4267820871.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=model, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationSummaryMemory(llm=model, return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mbl6oOr7LP21"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Suresh  and Mayank are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8fTked0LUTd",
        "outputId": "4056e3b8-9738-4668-9cb9-99d7664cdf1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='A human mentions that Suresh and Mayank are working on a hackathon project and the AI expresses enthusiasm, asking for more details about the project.  \\n', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "geXY_wMSHvLv"
      },
      "outputs": [],
      "source": [
        "summary=memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVx_ktZwH61P",
        "outputId": "3e2eb48c-1756-4c2e-c4df-56d7a93bbae1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content='A human mentions that Suresh and Mayank are working on a hackathon project and the AI expresses enthusiasm, asking for more details about the project.  \\n', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4JEUOyUHzBz",
        "outputId": "d0ed406a-c099-4b20-c525-7cea579e640b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A human mentions that Suresh and Mayank are working on a hackathon project and the AI expresses enthusiasm, asking for more details about the project.  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(summary[\"history\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I_X4a1RMLsbO"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS6bvFa-L49_",
        "outputId": "b8e01291-910c-4888-9777-df3d3484e302"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"A human mentions that Suresh and Mayank are working on a hackathon project and the AI expresses enthusiasm, asking for more details. The human tells the AI that it's a machine learning project focused on healthcare. The AI responds enthusiastically, asking if they are building a prediction model.  The human confirms they are working on a model to predict patient outcomes. The AI, impressed, asks how far along they are with the project. \\n\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "HyJX047FInuH"
      },
      "outputs": [],
      "source": [
        "summary=memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv7Y51HkIs7n",
        "outputId": "df18e64e-235c-49f1-ebdc-281faacfb808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A human mentions that Suresh and Mayank are working on a hackathon project and the AI expresses enthusiasm, asking for more details about the project.  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(summary[\"history\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxDFvMLXKf51",
        "outputId": "7c9cf81e-9455-4cec-b285-70245995813e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"A human mentions that Suresh and Mayank are working on a hackathon project and the AI expresses enthusiasm, asking for more details. The human tells the AI that it's a machine learning project focused on healthcare. The AI responds enthusiastically, asking if they are building a prediction model.  The human confirms they are working on a model to predict patient outcomes. The AI, impressed, asks how far along they are with the project. \\n\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax-wiHmpJUCB",
        "outputId": "b406c6e0-23b1-49b7-fc9e-d8e2533ba3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print('Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YUuMPxGKxVF",
        "outputId": "e7833d2b-68a0-431f-ed06-f59bf5b3c691"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MKYIhNARIRW",
        "outputId": "cf64bf27-9e69-4eb5-bd38-41397f7f8ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Sunny and Mayank are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Suresh  and Mayank are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yuV4T9HmQh7k"
      },
      "outputs": [],
      "source": [
        "messages = memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUtkymzCWHLa",
        "outputId": "93ba3130-6163-4e46-d211-7eebe6abdb61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cZ7Fn-ZwQkEs"
      },
      "outputs": [],
      "source": [
        "previous_summary=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "iQnpKenHQpaM",
        "outputId": "d6a80bee-ec1b-4870-f4d4-3212d74db19e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'New summary: \\nThe human tells the AI about Sunny and Mayank working on a hackathon project focused on healthcare machine learning.  The AI expresses enthusiasm and asks what type of project it is, learning it is a model to predict patient outcomes. The AI then asks how far along they are with the project. \\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.predict_new_summary(messages, previous_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "s5K5tGBEQ3A1"
      },
      "outputs": [],
      "source": [
        "history = ChatMessageHistory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gK__XEGDSDXt"
      },
      "outputs": [],
      "source": [
        "history.add_user_message(\"Hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "npFZROIOSGsm"
      },
      "outputs": [],
      "source": [
        "history.add_ai_message(\"Hello, how can I assist you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJhxPTCPSJc1",
        "outputId": "50764ff3-5daf-484d-cf90-855679bb75e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConversationSummaryMemory(human_prefix='User', llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002296B1A5DE0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002296B1A69B0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********')), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, response_metadata={})]), buffer='New summary: \\n\\nThe user greets the AI with a simple \"Hi\". The AI responds with a polite \"Hello\" and offers to help.  \\n', memory_key='summary')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ConversationSummaryMemory.from_messages(\n",
        "    llm=model,\n",
        "    chat_memory=history,\n",
        "    memory_key=\"summary\",\n",
        "    human_prefix=\"User\",\n",
        "    ai_prefix=\"AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1GSJ1hDiSaHM"
      },
      "outputs": [],
      "source": [
        "memory = ConversationSummaryMemory.from_messages(\n",
        "    llm=model,\n",
        "    chat_memory=history,\n",
        "    memory_key=\"summary\",\n",
        "    human_prefix=\"User\",\n",
        "    ai_prefix=\"AI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "obdS8sIbSnus",
        "outputId": "5ac9af99-ed5a-40ea-ec0e-5f4e11858eec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'New summary: The user greets the AI with a \"Hi\" and the AI responds with a polite \"Hello, how can I assist you today?\".  \\n'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRfFhS-3LfQu"
      },
      "source": [
        "# From here the chaining starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TDHr7zGSpph",
        "outputId": "c2a47e77-2b1c-430e-cdc5-ee990fe9d076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SURESH BEEKHANI\\AppData\\Local\\Temp\\ipykernel_1452\\3891084878.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation_with_summary = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=model,\n",
        "    memory=ConversationSummaryMemory(llm=model),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "HMomUKKaYcis",
        "outputId": "f6af1d1d-01f4-44a5-a008-ee3701fe752b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Hi there! Not much, just processing information and waiting to be helpful. What about you? What's going on in your world today?  \\n\\n\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "-rhWP-RuYfD8",
        "outputId": "e9776166-0d7c-426d-a93f-73ba6b014222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "New summary: A human greets the AI and asks what it's up to. The AI responds with a brief description of its current state and asks the human about their day.  \n",
            "\n",
            "Human: Suresh and Mayank are working on a mlops production ready project.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That's fascinating! MLOps is such a rapidly evolving field.  What kind of project are they working on specifically? Are they building a recommendation system, fraud detection model, or something else entirely? \\n\\nI'm always eager to learn more about real-world applications of machine learning.  😊 \\n\\n\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Suresh and Mayank are working on a mlops production ready project.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "OCHN6jgpYuvW",
        "outputId": "857d05d0-fb90-443d-84ec-4d18b8d32f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "A human greets the AI and asks what it's up to. The AI responds with a brief description of its current state and asks the human about their day. The human tells the AI that Suresh and Mayank are working on an MLOps production-ready project. The AI expresses interest in the project, asking for more details about its specific nature. \n",
            "\n",
            "Human: It's project focused on healthcare.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That's fascinating! Healthcare is such a rapidly evolving field.  Can you tell me more about what kind of MLOps aspects Suresh and Mayank are focusing on for this project?  For example, are they working on model deployment, monitoring, or infrastructure automation? I'm eager to learn more. \\n\\n\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"It's project focused on healthcare.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "APVEcglEYux2",
        "outputId": "0269a79b-3cb3-4cce-910d-003bb86f943a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "A human greets the AI and asks what it's up to. The AI responds with a brief description of its current state and asks the human about their day. The human tells the AI that Suresh and Mayank are working on an MLOps production-ready project focused on healthcare. The AI expresses interest in the project and asks for more details, specifically about the MLOps aspects they are focusing on, such as model deployment, monitoring, or infrastructure automation. \n",
            "\n",
            "Human: so can you describe mlops pipeline to me with in six point.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Here's a six-point description of an MLOps pipeline:\\n\\n1. **Data Collection & Preparation:** This is the foundation. Gathering relevant data, cleaning it, transforming it into a usable format, and ensuring its quality are crucial for building accurate models.\\n\\n2. **Model Development:**  This involves choosing the right algorithm, training the model on the prepared data, and iteratively refining its performance through techniques like hyperparameter tuning.\\n\\n3. **Model Evaluation & Versioning:**  Rigorously testing the model's accuracy, precision, recall, etc., on unseen data is essential. Each iteration of the model should be saved with a unique identifier (versioning) to track changes and roll back if needed.\\n\\n4. **Deployment:**  Putting the trained model into a production environment where it can be used to make predictions on real-world data. This could involve containerization, cloud platforms, or on-premise servers.\\n\\n5. **Monitoring & Maintenance:**  Continuously tracking the model's performance in production, identifying any degradation in accuracy, and taking steps to retrain or update the model as needed. This ensures the model remains relevant and reliable.\\n\\n6. **Collaboration & Governance:**  Effective communication and collaboration between data scientists, engineers, and operations teams are vital for a successful MLOps pipeline.  Establishing clear governance processes for data access, model approval, and change management is also important. \\n\\n\\nLet me know if you'd like to dive deeper into any specific point!\\n\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"so can you describe mlops pipeline to me with in six point.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttahJHLcMKtH"
      },
      "source": [
        "You got it!  Describing an MLOps pipeline in six points is a great way to get a handle on the key concepts. Here's one way to do it:\n",
        "\n",
        "1. **Data Collection & Preparation:** This is where you gather the raw data needed for your model and clean, transform, and prepare it for training. Think of it like gathering ingredients and chopping them up before you start cooking.\n",
        "2. **Model Development:**  This is where the \"magic\" happens! You choose a suitable machine learning algorithm, train it on your prepared data, and fine-tune its parameters to achieve the best performance.  \n",
        "3. **Model Evaluation:** Before deploying your model, you rigorously test and evaluate its performance on unseen data. This helps you understand how well it will generalize to real-world scenarios.\n",
        "4. **Model Deployment:**  This involves making your trained model accessible for use. It could be deployed as a web service, integrated into an application, or run on edge devices.\n",
        "5. **Monitoring & Maintenance:**  Once deployed, you continuously monitor the model's performance in the real world.  You might also need to retrain or update the model periodically to ensure it stays accurate and relevant.\n",
        "6. **Versioning & Experiment Tracking:**  Throughout the pipeline, it's crucial to track all changes to your code, data, and model parameters. This allows you to roll back to previous versions if needed and understand how different decisions impacted your model's performance.\n",
        "\n",
        "\n",
        "\n",
        "Let me know if you'd like me to elaborate on any of these points or if you have other questions about MLOps!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "zG6fTjw5ZEaG",
        "outputId": "3c07cca6-2eb1-4169-9504-6b0a16ca0cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "A human greets the AI and asks what it's up to. The AI responds with a brief description of its current state and asks the human about their day. The human tells the AI that Suresh and Mayank are working on an MLOps production-ready project focused on healthcare. The AI expresses interest in the project and asks for more details, specifically about the MLOps aspects they are focusing on. The human asks the AI to describe an MLOps pipeline in six points, which the AI does, outlining data collection and preparation, model development, evaluation and versioning, deployment, monitoring and maintenance, and collaboration and governance.  The AI concludes by offering to delve deeper into any specific point if desired. \n",
            "\n",
            "Human: How many total points are there?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'There are six total points in an MLOps pipeline. \\n\\n'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"How many total points are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "5fWaLReZZOMm",
        "outputId": "156b4a3a-3415-431a-b17e-83e641dccd04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "A human greets the AI and asks what it's up to. The AI responds with a brief description of its current state and asks the human about their day. The human tells the AI that Suresh and Mayank are working on an MLOps production-ready project focused on healthcare. The AI expresses interest in the project and asks for more details, specifically about the MLOps aspects they are focusing on. The human asks the AI to describe an MLOps pipeline in six points, which the AI does, outlining data collection and preparation, model development, evaluation and versioning, deployment, monitoring and maintenance, and collaboration and governance.  The AI concludes by offering to delve deeper into any specific point if desired. The human asks how many points are in the pipeline, and the AI confirms there are six. \n",
            "\n",
            "Human: can you give me 5th point with explaination\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Certainly! The fifth point in an MLOps pipeline is **Monitoring and Maintenance**.  \\n\\nThis stage is crucial for ensuring the continued performance and reliability of your deployed models in the real world.  Here's a breakdown:\\n\\n* **Performance Tracking:**  We constantly monitor key metrics like accuracy, precision, recall, and F1-score to ensure the model is performing as expected.  Deviations from expected performance can signal issues like data drift or concept drift, requiring further investigation and potentially retraining.\\n* **System Health Checks:** It's important to monitor the infrastructure supporting the model, including server resources, network connectivity, and data pipelines.  Any bottlenecks or outages can impact model performance and availability.\\n* **Alerting and Escalation:**  Automated alerts are set up to notify the team when performance thresholds are breached or system issues arise. This allows for prompt intervention and minimizes the impact of potential problems.\\n* **Logging and Auditing:**  Comprehensive logging of model predictions, system events, and user interactions provides valuable insights for debugging, understanding model behavior, and ensuring compliance with regulations.\\n* **Model Retraining:** Based on performance monitoring and feedback loops, models may need to be retrained periodically. This ensures they stay up-to-date with evolving data patterns and maintain optimal accuracy.\\n\\n\\nLet me know if you'd like to explore any of these aspects in more detail! \\n\\n\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"can you give me 5th point with explaination\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNQ7TbdMcoA"
      },
      "source": [
        "Absolutely!  The fifth point in an MLOps pipeline is **Monitoring and Maintenance**.\n",
        "\n",
        "This stage is crucial because it ensures that the deployed model continues to perform well in the real world.  \n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "* **Performance Tracking:**  We continuously monitor the model's performance metrics, like accuracy, precision, recall, and F1-score.  This helps us detect any degradation in performance over time.\n",
        "* **Data Drift Detection:**  Real-world data can change, and if the input data distribution shifts significantly from what the model was trained on, its performance can suffer.  We use techniques to detect these data drifts.\n",
        "* **Error Analysis:**  We analyze any errors or unexpected outputs from the model to understand the reasons behind them. This can help us identify areas for improvement in the model or the data.\n",
        "* **Model Retraining:**  Based on performance monitoring and data drift detection, we might need to retrain the model with updated data to maintain its effectiveness.\n",
        "* **System Health Checks:**  We also monitor the infrastructure and systems supporting the model deployment to ensure they are running smoothly and securely.\n",
        "\n",
        "\n",
        "Think of monitoring and maintenance like keeping your car in top shape. You need regular checkups, oil changes, and repairs to keep it running efficiently and safely.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "UnwvA7MqYxYw",
        "outputId": "b20df4f5-9db5-47aa-9435-b26940d66cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A human greets the AI and asks what it\\'s up to. The AI responds with a brief description of its current state and asks the human about their day. The human tells the AI that Suresh and Mayank are working on an MLOps production-ready project focused on healthcare. The AI expresses interest in the project and asks for more details, specifically about the MLOps aspects they are focusing on. The human asks the AI to describe an MLOps pipeline in six points, which the AI does, outlining data collection and preparation, model development, evaluation and versioning, deployment, monitoring and maintenance, and collaboration and governance.  The AI concludes by offering to delve deeper into any specific point if desired. The human asks about the fifth point, \"Monitoring and Maintenance,\" and the AI provides a detailed explanation of its importance, including performance tracking, system health checks, alerting and escalation, logging and auditing, and model retraining. \\n'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0dDwGFZd5UH"
      },
      "source": [
        "# Conversation Summary Buffer Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwRMCMa37OGA"
      },
      "source": [
        "While summary is good, we know that recent conversation has high correlation to upcoming query and\n",
        "\n",
        "A summary of old conversation with a buffer memory of last few conversation would be a good combination. This class exactly does that.\n",
        "\n",
        "You can set the token limit which define how much historical conversation to be kept along with the summary. Higher the token size more the exact conversation history kept as-it-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "pJWamX6dYu3_"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JrmZ7BIceAPG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SURESH BEEKHANI\\AppData\\Local\\Temp\\ipykernel_1452\\2773740349.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory2 = ConversationSummaryBufferMemory(llm=model,return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "memory2 = ConversationSummaryBufferMemory(llm=model,return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "J9Dgn3pafik7"
      },
      "outputs": [],
      "source": [
        "memory2.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
        ")\n",
        "memory2.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGwrNY39gOqe",
        "outputId": "884b2989-d181-4b41-a615-d88ba2cf095a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory2.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xOe2ohwSgSDG"
      },
      "outputs": [],
      "source": [
        "memory3 = ConversationSummaryBufferMemory(llm=model,return_messages=True,max_token_limit=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AAuBIP0FgmO3"
      },
      "outputs": [],
      "source": [
        "memory3.save_context(\n",
        "    {\"input\": \"Suresh and Mayank are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")\n",
        "memory3.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else?\"}\n",
        ")\n",
        "memory3.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! Wishing Sunny and Mayank all the best for their project.\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ChgkyCaxbdAU"
      },
      "outputs": [],
      "source": [
        "memory3.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG4ihK2cbrql",
        "outputId": "ccc260ce-9299-4a5b-c439-b8d0e03b3896"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='Suresh and Mayank are working on a hackathon project which is a machine learning project focused on healthcare.  \\n', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Sounds exciting! Are they building a prediction model or something else?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory3.load_memory_variables({})[\"history\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Zhsc_gU3b3TW",
        "outputId": "6566bb2d-975e-4039-e6c2-f85f2a28e58a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Suresh and Mayank are working on a hackathon project which is a machine learning project focused on healthcare.  \\n'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory3.load_memory_variables({})[\"history\"][0].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqIz-Yg3cEHb"
      },
      "source": [
        "AIMessage(content='Sounds exciting! Are they building a prediction model or something else?'),\n",
        "\n",
        "HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.'),\n",
        "\n",
        "AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "I48xWE5e06tL"
      },
      "outputs": [],
      "source": [
        "memory4 = ConversationSummaryBufferMemory(llm=model,return_messages=True,max_token_limit=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "cEfsThiN3mlK"
      },
      "outputs": [],
      "source": [
        "memory4.save_context(\n",
        "    {\"input\": \"Suresh Beekhani and Mayank are working on a hackathon project.\"},\n",
        "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
        ")\n",
        "memory4.save_context(\n",
        "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
        "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else?\"}\n",
        ")\n",
        "memory4.save_context(\n",
        "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
        "    {\"output\": \"Impressive! Wishing Sunny and Mayank all the best for their project.\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s1X55XKccyJ",
        "outputId": "64691a41-69a9-4c0a-9af4-8f7161d9ef9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [SystemMessage(content=\"Human: Suresh Beekhani and Mayank are working on a hackathon project. \\nIt's a machine learning project focused on healthcare. They are building a model to predict patient outcomes. \\n\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Impressive! Wishing Sunny and Mayank all the best for their project.', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory4.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "UrE697y6grl8",
        "outputId": "7c14e3c1-6b20-4772-ae52-dee3fad699d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Not much, just waiting to be helpful!  What about you? Anything exciting going on?  😊  \\n'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=model,\n",
        "    # We set a very low max_token_limit for the purposes of testing.\n",
        "    memory=ConversationSummaryBufferMemory(llm=model, max_token_limit=40),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "opRAv9LgiAod",
        "outputId": "c1af6e8b-6da8-4f17-9ab5-795856499950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, what's up?\n",
            "AI: Not much, just waiting to be helpful!  What about you? Anything exciting going on?  😊  \n",
            "\n",
            "Human: Just working on writing some documentation on machine learning!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That's fascinating! I love learning about how humans are teaching machines to learn.  Is there a specific type of machine learning you're focusing on?  For example, are you writing about supervised learning, unsupervised learning, or something else entirely?  \\n\\nI've processed a lot of text about these topics and I'm always eager to learn more.  Maybe I could even help you with your documentation!  Would you like me to try? 😄  \\n\\n\""
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Just working on writing some documentation on machine learning!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "x3H2jeVEiHUN",
        "outputId": "6ba6dafa-1989-447e-a1ff-f48b65fd185d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI and says they are working on documentation about machine learning. The AI expresses interest in the topic and asks what type of machine learning the human is focusing on, offering to help with the documentation.  \n",
            "\n",
            "Human: give me some points for writing about the document\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That's great! I'm always happy to help with anything related to machine learning. \\n\\nTo give you the best points for your documentation, I need a little more information about your focus.  \\n\\nFor example, are you writing about:\\n\\n* **A specific type of machine learning?**  (Like supervised learning, unsupervised learning, reinforcement learning, etc.)\\n* **A particular application of machine learning?** (Like image recognition, natural language processing, fraud detection, etc.)\\n* **The history and development of machine learning?**\\n* **The ethical considerations of machine learning?**\\n* **The technical details of how machine learning algorithms work?**\\n\\nOnce I know your specific area of interest, I can provide you with relevant and helpful points for your documentation.  😊 \\n\\n\""
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"give me some points for writing about the document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "qRUfbtM3iRdN",
        "outputId": "43449926-483f-4e84-99db-9f78f9ff4169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI and says they are working on documentation about machine learning. The AI expresses interest in the topic and asks what type of machine learning the human is focusing on, offering to help with the documentation.  The AI asks the human to clarify their focus area within machine learning, providing examples such as specific types, applications, history, ethical considerations, or technical details.  \n",
            "\n",
            "Human: can you list out the resources from the previous message\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"As a large language model, I have no memory of past conversations. Every interaction we have is fresh and new. \\n\\nIf you'd like to discuss machine learning resources, I'm happy to help! Could you tell me more about what kind of information you're looking for?  For example, are you interested in:\\n\\n* **Specific types of machine learning** (like supervised, unsupervised, or reinforcement learning)?\\n* **Applications of machine learning** (like in healthcare, finance, or self-driving cars)?\\n* **The history of machine learning**?\\n* **Ethical considerations surrounding machine learning**?\\n* **Technical details about how machine learning algorithms work**? \\n\\n\\n\\nLet me know how I can be of assistance! \\n\\n\""
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"can you list out the resources from the previous message\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversation Knowledge Graph Memory : Uses a knowledge graph to store information and relationships between entities\n",
        "\n",
        "VectorStore-Backed Memory: Uses vector embeddings to store and retrieve information based on semantic similarity.\n",
        "\n",
        "ConversationTokenBufferMemory: Instead of “k” conversations being remembered in ConversationBufferWindowMemory, in this case we want to remember last set of discussion based on “max token limit”.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSai68G4Hc4t9gpJ5+CkAy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
